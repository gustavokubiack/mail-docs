# Provisionamento

## Ferramenta Utilizada

Para o provisionamento da infraestrutura, foi utilizada a ferramenta **Terraform**, uma das mais robustas e populares solu√ß√µes de Infrastructure as Code (IaC). O Terraform permite definir, versionar e aplicar configura√ß√µes de infraestrutura de forma declarativa, com total controle e auditabilidade.

## Estrutura de Provisionamento

A seguir, uma descri√ß√£o das principais partes do c√≥digo Terraform utilizado:

### üî∏ VPC e Subnets

Foi criada uma **VPC personalizada** com suporte a DNS e hostname, subdividida em:

- **Subnets p√∫blicas:** destinadas ao cluster EKS.
- **Subnets privadas:** reservadas para o banco de dados PostgreSQL no RDS.

Tamb√©m foi configurado um **Internet Gateway** e uma **Route Table** para roteamento externo das subnets p√∫blicas.

### üî∏ IAM Roles e Policies

Duas roles principais foram configuradas:

- **eks_cluster_role**: utilizada pelo plano de controle do EKS com a policy `AmazonEKSClusterPolicy`.
- **eks_node_role**: atribu√≠da aos n√≥s do cluster com as seguintes policies:
  - `AmazonEKSWorkerNodePolicy`
  - `AmazonEKS_CNI_Policy`
  - `AmazonEC2ContainerRegistryReadOnly`

### üî∏ Cluster EKS

O cluster Kubernetes foi criado com o nome `mail-cluster`, configurado para usar as subnets p√∫blicas da VPC, com autentica√ß√£o baseada em IAM Roles.

### üî∏ Node Group

Foi criado um grupo de n√≥s com:
- **5 inst√¢ncias EC2 `t3.micro`**
- **Auto scaling fixo (min = 1, max = 5, desired = 5)**
- Tipo de inst√¢ncia compat√≠vel com o Free Tier da AWS.

Esses n√≥s foram associados √† role `eks_node_role` para comunica√ß√£o adequada com o EKS e demais servi√ßos.

### üî∏ Banco de Dados (RDS)

Um banco de dados **PostgreSQL** foi provisionado usando `aws_db_instance` com:
- **20 GB de armazenamento**
- Inst√¢ncia `db.t3.micro`
- Subnets privadas e um **Security Group** configurado para permitir apenas acesso interno pela VPC (porta 5432).

### üî∏ Helm Release (ArgoCD)

O ArgoCD foi instalado via Helm no cluster EKS. A release foi definida como `argocd`, utilizando o reposit√≥rio oficial do projeto Argo Helm:

- Namespace: `argocd`
- Chart: `argo-cd`
- Vers√£o: `5.46.7`
- Arquivo customizado: `argocd-values.yaml`

### üî∏ Kubernetes Secrets

Foi criado um `Secret` Kubernetes com todas as vari√°veis sens√≠veis da aplica√ß√£o, como:

- Dados do banco (host, user, senha)
- Configura√ß√µes de e-mail
- Flags de debug
- Informa√ß√µes de autentica√ß√£o SMTP

## Desafios e Solu√ß√µes

### üî∏ Capacidade dos N√≥s e Limite de Pods

Inicialmente, o cluster foi configurado com **apenas 1 n√≥ `t3.micro`**, o que rapidamente gerou problemas de capacidade. Isso se deve ao fato de que:

- A AWS executa **pods internos no cluster**, como o `CoreDNS`, `kube-proxy`, `aws-node`, entre outros.
- A inst√¢ncia `t3.micro` tem um limite baixo de **pods por n√≥** (aproximadamente 11 a 17, dependendo da configura√ß√£o da ENI).
- A aplica√ß√£o, ArgoCD e servi√ßos auxiliares tamb√©m consomem pods.

### üîß Solu√ß√£o

Para resolver, foi realizado o **aumento do n√∫mero de n√≥s de 1 para 5**, garantindo maior disponibilidade de recursos para os pods do sistema e da aplica√ß√£o, al√©m de permitir uma opera√ß√£o mais pr√≥xima da realidade de produ√ß√£o.

Esse ajuste resolveu os problemas de scheduling e permitiu a instala√ß√£o e opera√ß√£o est√°vel do ArgoCD e da aplica√ß√£o.

---

### V√≠deo de Demonstra√ß√£o

<iframe width="560" height="315" src="https://www.youtube.com/embed/4deqtnkoYVE?si=Go64BnngdnCZ4SYl" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
